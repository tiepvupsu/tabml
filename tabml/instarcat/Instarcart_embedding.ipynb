{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4686089d",
   "metadata": {},
   "source": [
    "# Create product embedding for instacart dataset\n",
    "\n",
    "Idea: similar to word2vec, products in the same order are close to each other\n",
    "\n",
    "## Load orders and product name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06530aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aisles.csv\t\t       order_products__train.csv.zip\r\n",
      "aisles.csv.zip\t\t       orders.csv\r\n",
      "departments.csv.zip\t       orders.csv.zip\r\n",
      "__MACOSX\t\t       products.csv\r\n",
      "order_products__prior.csv      products.csv.zip\r\n",
      "order_products__prior.csv.zip  sample_submission.csv\r\n",
      "order_products__train.csv      sample_submission.csv.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd5ed7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "product_df = pd.read_csv(\"data/products.csv\")\n",
    "order_df = pd.read_csv(\"data/orders.csv\")\n",
    "order_prior_df = pd.read_csv(\"data/order_products__prior.csv\")\n",
    "# print(product_df.head(10))\n",
    "# print(order_df.head(10))\n",
    "# print(order_prior_df.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d42dee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products: 49688\n",
      "Number of orders: 3214874\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of products: {len(product_df)}\")\n",
    "print(f\"Number of orders: {len(order_prior_df['order_id'].value_counts())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61b1e8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of orders with at least 2 products: 20\n",
      "Number of reduced products: 1308\n"
     ]
    }
   ],
   "source": [
    "# Filter our order with less than 2 products\n",
    "df1 = order_prior_df[order_prior_df.groupby(\"order_id\")['order_id'].transform('size') > 100]\n",
    "print(f\"Number of orders with at least 2 products: {len(df1['order_id'].value_counts())}\")\n",
    "\n",
    "# Get list of products in these orders\n",
    "product_ids = df1[\"product_id\"].unique().tolist()\n",
    "product_df1 = product_df[product_df[\"product_id\"].isin(product_ids)]\n",
    "print(f\"Number of reduced products: {len(product_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f42b997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build product mapping\n",
    "\n",
    "product_ids = product_df1[\"product_id\"]\n",
    "product_names = product_df1[\"product_name\"]\n",
    "# map product id to indices starting from 0\n",
    "product_id_to_ind = {product_id: i for i, product_id in enumerate(product_ids)}\n",
    "product_ind_to_name = {i: product_name for i, product_name in enumerate(product_names)}\n",
    "product_name_to_ind = {product_name: i for i, product_name in enumerate(product_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9981f945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2242it [00:00, 2701416.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# torch dataset\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tqdm\n",
    "\n",
    "class ProductPairDataset(Dataset):\n",
    "    \"\"\"Dataset class that returns a pair of (context, target) product ids.\n",
    "    \n",
    "    The pair is a random combination of 2 products in the same order.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, order_df, product_id_to_ind, num_noisy_products: int = 20):\n",
    "        \n",
    "        self.id_to_ind = product_id_to_ind\n",
    "        unique_orders = order_df[\"order_id\"].unique().tolist()\n",
    "        order_to_ind = {\n",
    "            order_id: i\n",
    "            for i, order_id in enumerate(unique_orders)\n",
    "        }\n",
    "        # self.orders to store a list of orders, each order is represented by a list of products in that order\n",
    "        self.orders = [[] for _ in unique_orders]\n",
    "        for order_id, product_id in tqdm.tqdm(zip(order_df[\"order_id\"], order_df[\"product_id\"])):\n",
    "            order_ind = order_to_ind[order_id]\n",
    "            product_ind = product_id_to_ind[product_id]\n",
    "            self.orders[order_ind].append(product_ind)\n",
    "        self.num_noisy_products = num_noisy_products\n",
    "        self.num_products = len(self.id_to_ind)        \n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        # The true number of pairs is much higher than this, but it does not really matter\n",
    "        # as long as we train the model long enough.\n",
    "        return len(order_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get a random order and a random pair of products in that order\n",
    "        products = random.choice(self.orders)\n",
    "        selected_products = random.sample(products, 2)\n",
    "#         print(selected_products)\n",
    "#         print(self.num_products)\n",
    "        context_product_ind = selected_products[0]\n",
    "        target_product_ind = selected_products[1]\n",
    "        # buid mask\n",
    "        mask = [0 for _ in range(self.num_products)]\n",
    "        random_ids = random.sample(range(self.num_products), self.num_noisy_products)\n",
    "        for i in random_ids + [context_product_ind]:\n",
    "            mask[i] = 1\n",
    "        return context_product_ind, target_product_ind, mask\n",
    "        \n",
    "training_data = ProductPairDataset(df1, product_id_to_ind)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09accafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "train_dataloader = DataLoader(\n",
    "    training_data, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8249cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pytorch lightning model\n",
    "import pytorch_lightning as pl\n",
    "import torch.multiprocessing\n",
    "import torch.nn.functional as F\n",
    "class SigmoidBCELoss(nn.Module):\n",
    "    \"BCEWithLogitLoss with masking on call.\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, target, mask=None):\n",
    "        out = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, target, weight=mask, reduction=\"none\")\n",
    "        return out.mean(dim=1)\n",
    "\n",
    "loss_fn = SigmoidBCELoss()\n",
    "loss2 = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "class Prod2VecModel(pl.LightningModule):\n",
    "    def __init__(self, num_products, embed_size: int = 50):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding = nn.Embedding(num_products, self.embed_size)\n",
    "        self.hidden = nn.Linear(self.embed_size, num_products, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, contexts):\n",
    "#         breakpoint()\n",
    "        hid = self.embedding(contexts)\n",
    "        pre_sigmoid = self.hidden(hid)\n",
    "        return pre_sigmoid\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        contexts, targets, masks = batch\n",
    "        breakpoint()\n",
    "        output = self.forward(contexts)\n",
    "        # loss = loss_fn(output, targets, masks)\n",
    "        loss = loss2(output, targets)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(), lr=0.5, momentum=0.5, weight_decay=1e-3\n",
    "        )  # learning rate\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d0ee01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | embedding | Embedding | 2.6 K \n",
      "1 | hidden    | Linear    | 2.6 K \n",
      "----------------------------------------\n",
      "5.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04e6f13cb5346e782d7051cf355704c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in sys.unraisablehookException ignored in sys.unraisablehook: : <built-in function unraisablehook><built-in function unraisablehook>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tiepvu/w/tabml/tabml_env/lib/python3.8/site-packages/ipykernel/iostream.py\", line 410, in write\n",
      "  File \"/home/tiepvu/w/tabml/tabml_env/lib/python3.8/site-packages/ipykernel/iostream.py\", line 410, in write\n",
      "        self.pub_thread.schedule(self._flush)self.pub_thread.schedule(self._flush)\n",
      "\n",
      "  File \"/home/tiepvu/w/tabml/tabml_env/lib/python3.8/site-packages/ipykernel/iostream.py\", line 205, in schedule\n",
      "  File \"/home/tiepvu/w/tabml/tabml_env/lib/python3.8/site-packages/ipykernel/iostream.py\", line 205, in schedule\n",
      "        f()f()\n",
      "\n",
      "  File \"/home/tiepvu/w/tabml/tabml_env/lib/python3.8/site-packages/ipykernel/iostream.py\", line 381, in _flush\n",
      "  File \"/home/tiepvu/w/tabml/tabml_env/lib/python3.8/site-packages/ipykernel/iostream.py\", line 381, in _flush\n",
      "        self.session.send(self.pub_thread, 'stream', content=content,self.session.send(self.pub_thread, 'stream', content=content,\n",
      "\n",
      "  File \"/home/tiepvu/w/tabml/tabml_env/lib/python3.8/site-packages/jupyter_client/session.py\", line 753, in send\n",
      "  File \"/home/tiepvu/w/tabml/tabml_env/lib/python3.8/site-packages/jupyter_client/session.py\", line 753, in send\n",
      "        stream.send_multipart(to_send, copy=copy)stream.send_multipart(to_send, copy=copy)\n",
      "\n",
      "  File \"/home/tiepvu/w/tabml/tabml_env/lib/python3.8/site-packages/ipykernel/iostream.py\", line 212, in send_multipart\n",
      "  File \"/home/tiepvu/w/tabml/tabml_env/lib/python3.8/site-packages/ipykernel/iostream.py\", line 212, in send_multipart\n",
      "        self.schedule(lambda : self._really_send(*args, **kwargs))self.schedule(lambda : self._really_send(*args, **kwargs))\n",
      "\n",
      "  File \"/home/tiepvu/w/tabml/tabml_env/lib/python3.8/site-packages/ipykernel/iostream.py\", line 205, in schedule\n",
      "  File \"/home/tiepvu/w/tabml/tabml_env/lib/python3.8/site-packages/ipykernel/iostream.py\", line 205, in schedule\n",
      "        f()f()\n",
      "  File \"/home/tiepvu/w/tabml/tabml_env/lib/python3.8/site-packages/ipykernel/iostream.py\", line 212, in <lambda>\n",
      "\n",
      "      File \"/home/tiepvu/w/tabml/tabml_env/lib/python3.8/site-packages/ipykernel/iostream.py\", line 212, in <lambda>\n",
      "self.schedule(lambda : self._really_send(*args, **kwargs))    \n",
      "  File \"/home/tiepvu/w/tabml/tabml_env/lib/python3.8/site-packages/ipykernel/iostream.py\", line 225, in _really_send\n",
      "self.schedule(lambda : self._really_send(*args, **kwargs))    \n",
      "ctx, pipe_out = self._setup_pipe_out()  File \"/home/tiepvu/w/tabml/tabml_env/lib/python3.8/site-packages/ipykernel/iostream.py\", line 225, in _really_send\n",
      "\n",
      "      File \"/home/tiepvu/w/tabml/tabml_env/lib/python3.8/site-packages/ipykernel/iostream.py\", line 153, in _setup_pipe_out\n",
      "ctx, pipe_out = self._setup_pipe_out()    \n",
      "ctx = zmq.Context()  File \"/home/tiepvu/w/tabml/tabml_env/lib/python3.8/site-packages/ipykernel/iostream.py\", line 153, in _setup_pipe_out\n",
      "\n",
      "  File \"zmq/backend/cython/context.pyx\", line 48, in zmq.backend.cython.context.Context.__cinit__\n",
      "    zmq.errorctx = zmq.Context().\n",
      "ZMQError  File \"zmq/backend/cython/context.pyx\", line 48, in zmq.backend.cython.context.Context.__cinit__\n",
      ": zmq.errorToo many open files.\n",
      "ZMQError: Too many open files\n"
     ]
    }
   ],
   "source": [
    "num_products = len(product_id_to_ind)\n",
    "embed_size = 2\n",
    "model = Prod2VecModel(num_products, embed_size)\n",
    "trainer = pl.Trainer(gpus=0, max_epochs=1)\n",
    "trainer.fit(model, train_dataloader, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cfbc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3edfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34796195",
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3576952",
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba25f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[] for i in range(3)]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e8ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0].append(1)\n",
    "a[1].append(2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8051e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df1.iloc[0][\"product_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda9ef3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabml_env",
   "language": "python",
   "name": "tabml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
